"""
–ú–æ–¥—É–ª—å 2.3: –†–æ–±–æ—Ç–∞ –∑ —Ä—è–¥–∫–∞–º–∏ –¥–ª—è Data Science/Engineering
==========================================================

String processing, regex, data cleaning patterns
"""

import re
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import unicodedata
from typing import Any

# ============================================================================
# 1. REGEX - DATA CLEANING
# ============================================================================

print("=" * 70)
print("1. –†–ï–ì–£–õ–Ø–†–ù–Ü –í–ò–†–ê–ó–ò - DATA CLEANING")
print("=" * 70)

def clean_phone_number(phone: str) -> str | None:
    """
    –û—á–∏—â—É—î —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–∏–π –Ω–æ–º–µ—Ä
    
    Patterns:
        +380501234567
        050-123-45-67
        (050) 123 45 67
        0501234567
    
    Returns:
        +380501234567 –∞–±–æ None
    """
    # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –Ω–µ—á–∏—Å–ª–æ–≤—ñ —Å–∏–º–≤–æ–ª–∏ –∫—Ä—ñ–º +
    cleaned = re.sub(r'[^\d+]', '', phone)
    
    # –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ –Ω–æ–º–µ—Ä–∏
    pattern = r'^(\+380|380|0)(\d{9})$'
    match = re.match(pattern, cleaned)
    
    if match:
        prefix, number = match.groups()
        return f"+380{number}"
    
    return None


# –¢–µ—Å—Ç–∏
test_phones = [
    "+380501234567",
    "050-123-45-67",
    "(050) 123 45 67",
    "0501234567",
    "invalid"
]

print("–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–∏—Ö –Ω–æ–º–µ—Ä—ñ–≤:")
for phone in test_phones:
    cleaned = clean_phone_number(phone)
    status = "‚úÖ" if cleaned else "‚ùå"
    print(f"  {status} '{phone}' ‚Üí {cleaned}")


def extract_emails(text: str) -> list[str]:
    """
    –í–∏—Ç—è–≥—É—î email –∞–¥—Ä–µ—Å–∏ –∑ —Ç–µ–∫—Å—Ç—É
    –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è: web scraping, log analysis
    """
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return re.findall(pattern, text)


text_sample = """
–ö–æ–Ω—Ç–∞–∫—Ç–∏:
john.doe@example.com
support@company.co.uk
admin@test-server.com
invalid@email (—Ü–µ –Ω–µ email)
"""

emails = extract_emails(text_sample)
print(f"\n–ó–Ω–∞–π–¥–µ–Ω—ñ email –∞–¥—Ä–µ—Å–∏: {emails}")


# ============================================================================
# 2. DATA NORMALIZATION - CLEANING USER INPUT
# ============================================================================

print("\n" + "=" * 70)
print("2. –ù–û–†–ú–ê–õ–Ü–ó–ê–¶–Ü–Ø –î–ê–ù–ò–•")
print("=" * 70)

def normalize_company_name(name: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î –Ω–∞–∑–≤–∏ –∫–æ–º–ø–∞–Ω—ñ–π –¥–ª—è –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó
    
    –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è:
    - Data matching
    - Entity resolution
    - Deduplication
    """
    # Lowercase
    normalized = name.lower()
    
    # –í–∏–¥–∞–ª—è—î–º–æ —é—Ä–∏–¥–∏—á–Ω—ñ —Ñ–æ—Ä–º–∏
    legal_forms = [
        r'\b(llc|ltd|inc|corp|gmbh|sa|ag|nv|bv|plc)\b\.?',
        r'\b(limited|incorporated|corporation)\b',
        r'\b(—Ç–æ–≤–∞—Ä–∏—Å—Ç–≤–æ –∑ –æ–±–º–µ–∂–µ–Ω–æ—é –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—é|—Ç–æ–≤|—Ç–∑–æ–≤)\b'
    ]
    for pattern in legal_forms:
        normalized = re.sub(pattern, '', normalized, flags=re.IGNORECASE)
    
    # –í–∏–¥–∞–ª—è—î–º–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∏ —Ç–∞ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
    normalized = re.sub(r'[^\w\s]', '', normalized)
    normalized = ' '.join(normalized.split())
    
    return normalized.strip()


# –¢–µ—Å—Ç–∏
companies = [
    "Apple Inc.",
    "Apple Incorporated",
    "APPLE",
    "Google LLC",
    "Google",
    "Microsoft Corporation",
    "Microsoft Corp."
]

print("–ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞–∑–≤ –∫–æ–º–ø–∞–Ω—ñ–π:")
for company in companies:
    normalized = normalize_company_name(company)
    print(f"  '{company}' ‚Üí '{normalized}'")


# ============================================================================
# 3. PARSING STRUCTURED STRINGS
# ============================================================================

print("\n" + "=" * 70)
print("3. –ü–ê–†–°–ò–ù–ì –°–¢–†–£–ö–¢–£–†–û–í–ê–ù–ò–• –†–Ø–î–ö–Ü–í")
print("=" * 70)

def parse_log_line(log: str) -> dict | None:
    """
    –ü–∞—Ä—Å–∏—Ç—å —Ä—è–¥–æ–∫ –∑ –ª–æ–≥ —Ñ–∞–π–ª—É
    
    Format: [timestamp] LEVEL: message
    Example: [2024-10-23 14:30:45] ERROR: Database connection failed
    """
    pattern = r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\] (\w+): (.+)'
    match = re.match(pattern, log)
    
    if match:
        timestamp_str, level, message = match.groups()
        return {
            'timestamp': datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S'),
            'level': level,
            'message': message
        }
    return None


# –¢–µ—Å—Ç–∏
log_lines = [
    "[2024-10-23 14:30:45] ERROR: Database connection failed",
    "[2024-10-23 14:31:12] INFO: Processing batch 1000",
    "[2024-10-23 14:32:00] WARNING: High memory usage",
    "Invalid log line"
]

print("–ü–∞—Ä—Å–∏–Ω–≥ log —Ñ–∞–π–ª—ñ–≤:")
for log in log_lines:
    parsed = parse_log_line(log)
    if parsed:
        print(f"  ‚úÖ {parsed['level']}: {parsed['message'][:30]}...")
    else:
        print(f"  ‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏: {log}")


def parse_url_query(url: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ URL
    –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è: API logs, web analytics
    """
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ list –∑–Ω–∞—á–µ–Ω–Ω—è –≤ single values
    return {k: v[0] if len(v) == 1 else v for k, v in params.items()}


# –¢–µ—Å—Ç
url = "https://api.example.com/search?q=python&category=programming&limit=10&tags=ml&tags=ai"
params = parse_url_query(url)

print(f"\nURL: {url}")
print("–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:")
for key, value in params.items():
    print(f"  {key}: {value}")


# ============================================================================
# 4. TEXT PREPROCESSING - NLP BASICS
# ============================================================================

print("\n" + "=" * 70)
print("4. TEXT PREPROCESSING –î–õ–Ø NLP")
print("=" * 70)

def preprocess_text(text: str, lowercase: bool = True) -> dict:
    """
    –ë–∞–∑–æ–≤–∏–π preprocessing —Ç–µ–∫—Å—Ç—É
    
    Steps:
    1. Lowercase (optional)
    2. Remove punctuation
    3. Remove extra whitespace
    4. Tokenize
    5. Remove short words
    """
    result = {}
    
    # Original
    result['original'] = text
    
    # Lowercase
    if lowercase:
        text = text.lower()
    result['lowercased'] = text
    
    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)
    result['no_urls'] = text
    
    # Remove mentions and hashtags (–¥–ª—è —Å–æ—Ü –º–µ—Ä–µ–∂)
    text = re.sub(r'@\w+|#\w+', '', text)
    result['no_mentions'] = text
    
    # Remove punctuation
    text = re.sub(r'[^\w\s]', ' ', text)
    result['no_punctuation'] = text
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    result['cleaned'] = text
    
    # Tokenize
    tokens = text.split()
    result['tokens'] = tokens
    
    # Remove short words (< 3 chars)
    long_tokens = [t for t in tokens if len(t) >= 3]
    result['filtered_tokens'] = long_tokens
    
    # Stats
    result['word_count'] = len(tokens)
    result['unique_words'] = len(set(tokens))
    
    return result


# –¢–µ—Å—Ç
tweet = """
Check out this amazing #Python tutorial! üöÄ
https://example.com/tutorial @DataScience
It's really helpful for ML beginners!!!
"""

processed = preprocess_text(tweet)

print("Text Preprocessing:")
print(f"Original: {processed['original'][:50]}...")
print(f"Cleaned: {processed['cleaned']}")
print(f"Tokens: {processed['tokens'][:5]}...")
print(f"Word count: {processed['word_count']}")
print(f"Unique words: {processed['unique_words']}")


# ============================================================================
# 5. DATA VALIDATION WITH REGEX
# ============================================================================

print("\n" + "=" * 70)
print("5. –í–ê–õ–Ü–î–ê–¶–Ü–Ø –î–ê–ù–ò–•")
print("=" * 70)

class DataValidator:
    """–í–∞–ª—ñ–¥–∞—Ç–æ—Ä –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –¥–∞–Ω–∏—Ö"""
    
    @staticmethod
    def is_valid_email(email: str) -> bool:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è email"""
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, email))
    
    @staticmethod
    def is_valid_ip(ip: str) -> bool:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è IP –∞–¥—Ä–µ—Å–∏"""
        pattern = r'^(\d{1,3}\.){3}\d{1,3}$'
        if not re.match(pattern, ip):
            return False
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥—ñ–∞–ø–∞–∑–æ–Ω—É
        parts = ip.split('.')
        return all(0 <= int(part) <= 255 for part in parts)
    
    @staticmethod
    def is_valid_date(date_str: str, format: str = '%Y-%m-%d') -> bool:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞—Ç–∏"""
        try:
            datetime.strptime(date_str, format)
            return True
        except ValueError:
            return False
    
    @staticmethod
    def is_valid_credit_card(card: str) -> bool:
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –Ω–æ–º–µ—Ä–∞ –∫–∞—Ä—Ç–∫–∏ (Luhn algorithm)"""
        # –í–∏–¥–∞–ª—è—î–º–æ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –¥–µ—Ñ—ñ—Å–∏
        card = re.sub(r'[\s-]', '', card)
        
        if not card.isdigit() or len(card) not in [13, 15, 16]:
            return False
        
        # Luhn algorithm
        def luhn_checksum(card_number):
            def digits_of(n):
                return [int(d) for d in str(n)]
            
            digits = digits_of(card_number)
            odd_digits = digits[-1::-2]
            even_digits = digits[-2::-2]
            checksum = sum(odd_digits)
            for d in even_digits:
                checksum += sum(digits_of(d * 2))
            return checksum % 10
        
        return luhn_checksum(int(card)) == 0


# –¢–µ—Å—Ç–∏
validator = DataValidator()

test_data = {
    'emails': ['valid@example.com', 'invalid@', 'no-at-sign.com'],
    'ips': ['192.168.1.1', '255.255.255.255', '256.1.1.1', '192.168'],
    'dates': ['2024-10-23', '2024-13-45', '10/23/2024'],
    'cards': ['4532015112830366', '1234567890123456', '4532-0151-1283-0366']
}

print("Email –≤–∞–ª—ñ–¥–∞—Ü—ñ—è:")
for email in test_data['emails']:
    valid = validator.is_valid_email(email)
    print(f"  {'‚úÖ' if valid else '‚ùå'} {email}")

print("\nIP –∞–¥—Ä–µ—Å–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è:")
for ip in test_data['ips']:
    valid = validator.is_valid_ip(ip)
    print(f"  {'‚úÖ' if valid else '‚ùå'} {ip}")

print("\n–î–∞—Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è:")
for date in test_data['dates']:
    valid = validator.is_valid_date(date)
    print(f"  {'‚úÖ' if valid else '‚ùå'} {date}")


# ============================================================================
# 6. STRING SIMILARITY - DATA MATCHING
# ============================================================================

print("\n" + "=" * 70)
print("6. STRING SIMILARITY - DATA MATCHING")
print("=" * 70)

def levenshtein_distance(s1: str, s2: str) -> int:
    """
    –û–±—á–∏—Å–ª—é—î Levenshtein distance –º—ñ–∂ —Ä—è–¥–∫–∞–º–∏
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è:
    - Fuzzy matching
    - Spell checking
    - Record linkage
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            # Cost of insertions, deletions, or substitutions
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]


def similarity_ratio(s1: str, s2: str) -> float:
    """
    –û–±—á–∏—Å–ª—é—î similarity ratio [0, 1]
    1.0 = identical, 0.0 = completely different
    """
    distance = levenshtein_distance(s1.lower(), s2.lower())
    max_len = max(len(s1), len(s2))
    
    if max_len == 0:
        return 1.0
    
    return 1 - (distance / max_len)


# –¢–µ—Å—Ç–∏
print("String Similarity (Fuzzy Matching):")

# –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∫–æ–º–ø–∞–Ω—ñ–π
companies = [
    "Microsoft",
    "Microsft",  # Typo
    "Microsoft Corp",
    "Apple Inc",
    "Apple Incorporated"
]

threshold = 0.8
print(f"\n–ü–æ—à—É–∫ —Å—Ö–æ–∂–∏—Ö –Ω–∞–∑–≤ (threshold={threshold}):")

for i, company1 in enumerate(companies):
    for company2 in companies[i+1:]:
        similarity = similarity_ratio(company1, company2)
        if similarity >= threshold:
            print(f"  ‚ö†Ô∏è  '{company1}' ‚âà '{company2}' (similarity: {similarity:.2f})")


# ============================================================================
# 7. UNICODE HANDLING - INTERNATIONALIZATION
# ============================================================================

print("\n" + "=" * 70)
print("7. UNICODE –¢–ê –ú–Ü–ñ–ù–ê–†–û–î–ù–Ü –¢–ï–ö–°–¢–ò")
print("=" * 70)

def normalize_unicode(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É—î Unicode —Ç–µ–∫—Å—Ç
    –í–∏—Ä—ñ—à—É—î –ø—Ä–æ–±–ª–µ–º–∏ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è–º–∏ —Å–∏–º–≤–æ–ª—ñ–≤
    """
    # NFKD normalization - —Ä–æ–∑–∫–ª–∞–¥–∞—î —Å–∫–ª–∞–¥–Ω—ñ —Å–∏–º–≤–æ–ª–∏
    normalized = unicodedata.normalize('NFKD', text)
    
    # –í–∏–¥–∞–ª—è—î–º–æ combining characters (–¥—ñ–∞–∫—Ä–∏—Ç–∏—á–Ω—ñ –∑–Ω–∞–∫–∏)
    ascii_text = ''.join(c for c in normalized if not unicodedata.combining(c))
    
    return ascii_text


def remove_accents(text: str) -> str:
    """
    –í–∏–¥–∞–ª—è—î –∞–∫—Ü–µ–Ω—Ç–∏ (–¥–ª—è –ø–æ—à—É–∫—É)
    caf√© ‚Üí cafe
    """
    nfkd_form = unicodedata.normalize('NFKD', text)
    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])


# –¢–µ—Å—Ç–∏
international_texts = [
    "Caf√©",
    "na√Øve",
    "Z√ºrich",
    "–ú–æ—Å–∫–≤–∞",
    "Êó•Êú¨",  # –Ø–ø–æ–Ω—Å—å–∫–∞
    "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©"  # –ê—Ä–∞–±—Å—å–∫–∞
]

print("Unicode normalization:")
for text in international_texts:
    normalized = remove_accents(text)
    print(f"  '{text}' ‚Üí '{normalized}'")


# ============================================================================
# 8. STRING FORMATTING - REPORTS AND OUTPUTS
# ============================================================================

print("\n" + "=" * 70)
print("8. –§–û–†–ú–ê–¢–£–í–ê–ù–ù–Ø –î–õ–Ø –ó–í–Ü–¢–Ü–í")
print("=" * 70)

def format_table(data: list[dict], columns: list[str]) -> str:
    """
    –§–æ—Ä–º–∞—Ç—É—î –¥–∞–Ω—ñ –≤ ASCII —Ç–∞–±–ª–∏—Ü—é
    –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è CLI tools —Ç–∞ –∑–≤—ñ—Ç—ñ–≤
    """
    if not data:
        return "No data"
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É —à–∏—Ä–∏–Ω—É –¥–ª—è –∫–æ–∂–Ω–æ—ó –∫–æ–ª–æ–Ω–∫–∏
    col_widths = {col: len(col) for col in columns}
    
    for row in data:
        for col in columns:
            value = str(row.get(col, ''))
            col_widths[col] = max(col_widths[col], len(value))
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫
    separator = "+" + "+".join("-" * (col_widths[col] + 2) for col in columns) + "+"
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    header = "|" + "|".join(f" {col:{col_widths[col]}} " for col in columns) + "|"
    
    # –†—è–¥–∫–∏
    rows = []
    for row in data:
        row_str = "|" + "|".join(
            f" {str(row.get(col, '')):{col_widths[col]}} " for col in columns
        ) + "|"
        rows.append(row_str)
    
    # –ó–±–∏—Ä–∞—î–º–æ —Ç–∞–±–ª–∏—Ü—é
    table = [separator, header, separator]
    table.extend(rows)
    table.append(separator)
    
    return "\n".join(table)


# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
metrics_data = [
    {"model": "Random Forest", "accuracy": "0.92", "f1": "0.89"},
    {"model": "XGBoost", "accuracy": "0.94", "f1": "0.91"},
    {"model": "Neural Net", "accuracy": "0.96", "f1": "0.94"},
]

table = format_table(metrics_data, ["model", "accuracy", "f1"])
print("\nModel Performance Table:")
print(table)


def format_bytes(bytes_size: int) -> str:
    """–§–æ—Ä–º–∞—Ç—É—î —Ä–æ–∑–º—ñ—Ä –≤ —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π –≤–∏–≥–ª—è–¥"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.2f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.2f} PB"


# –¢–µ—Å—Ç
sizes = [500, 1500, 1_500_000, 1_500_000_000, 1_500_000_000_000]
print("\n–†–æ–∑–º—ñ—Ä–∏ —Ñ–∞–π–ª—ñ–≤:")
for size in sizes:
    print(f"  {size:>15,} bytes = {format_bytes(size)}")


# ============================================================================
# 9. TEMPLATE STRINGS - DYNAMIC CONTENT
# ============================================================================

print("\n" + "=" * 70)
print("9. STRING TEMPLATES - –î–ò–ù–ê–ú–Ü–ß–ù–ò–ô –ö–û–ù–¢–ï–ù–¢")
print("=" * 70)

def generate_sql_query(
    table: str,
    columns: list[str],
    conditions: dict[str, Any] | None = None
) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î SQL –∑–∞–ø–∏—Ç (simple templating)
    
    –í–ê–ñ–õ–ò–í–û: –£ production –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ parameterized queries!
    """
    cols = ", ".join(columns)
    query = f"SELECT {cols} FROM {table}"
    
    if conditions:
        where_clauses = []
        for key, value in conditions.items():
            if isinstance(value, str):
                where_clauses.append(f"{key} = '{value}'")
            else:
                where_clauses.append(f"{key} = {value}")
        
        query += " WHERE " + " AND ".join(where_clauses)
    
    return query + ";"


# –ü—Ä–∏–∫–ª–∞–¥
query = generate_sql_query(
    table="users",
    columns=["id", "name", "email"],
    conditions={"age": 25, "city": "Kyiv"}
)
print("Generated SQL:")
print(f"  {query}")


def generate_email_template(name: str, product: str, discount: int) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î email template
    """
    template = f"""
    –ü—Ä–∏–≤—ñ—Ç, {name}!
    
    –ú–∞—î–º–æ –¥–ª—è –≤–∞—Å —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—É –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—é –Ω–∞ {product}!
    –ó–Ω–∏–∂–∫–∞ {discount}% –¥—ñ—î —Ç—ñ–ª—å–∫–∏ —Å—å–æ–≥–æ–¥–Ω—ñ.
    
    –ù–µ –ø—Ä–æ–ø—É—Å—Ç—ñ—Ç—å!
    
    –ó –ø–æ–≤–∞–≥–æ—é,
    –ö–æ–º–∞–Ω–¥–∞ Store
    """
    return template.strip()


email = generate_email_template("–û–ª–µ–∫—Å–∞–Ω–¥—Ä", "Python Course", 30)
print("\nEmail Template:")
print(email)


# ============================================================================
# 10. PRACTICAL: DATA CLEANING PIPELINE
# ============================================================================

print("\n" + "=" * 70)
print("10. –ü–†–ê–ö–¢–ò–ö–ê: DATA CLEANING PIPELINE")
print("=" * 70)

def clean_text_pipeline(text: str) -> dict[str, Any]:
    """
    –ü–æ–≤–Ω–∏–π pipeline –æ—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
    """
    steps = {}
    
    # 1. Original
    steps['original'] = text
    steps['original_length'] = len(text)
    
    # 2. Strip whitespace
    text = text.strip()
    steps['stripped'] = text
    
    # 3. Normalize whitespace
    text = ' '.join(text.split())
    steps['whitespace_normalized'] = text
    
    # 4. Remove special characters (keep letters, numbers, spaces)
    text = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø—ñ–Ü—ó–á—î–Ñ0-9\s]', '', text)
    steps['special_chars_removed'] = text
    
    # 5. Lowercase
    text = text.lower()
    steps['lowercased'] = text
    
    # 6. Remove extra spaces again
    text = ' '.join(text.split())
    steps['final'] = text
    steps['final_length'] = len(text)
    
    # Stats
    steps['length_reduction'] = steps['original_length'] - steps['final_length']
    steps['reduction_pct'] = (steps['length_reduction'] / steps['original_length'] * 100) if steps['original_length'] > 0 else 0
    
    return steps


# –¢–µ—Å—Ç
dirty_text = """
    !!!  Hello   World!!!  
    This   is   a    TEST  @#$%  
    With  MANY    spaces!!!
"""

result = clean_text_pipeline(dirty_text)

print("Data Cleaning Pipeline:")
print(f"Original: '{result['original'][:50]}...'")
print(f"Final: '{result['final']}'")
print(f"Length: {result['original_length']} ‚Üí {result['final_length']}")
print(f"Reduction: {result['reduction_pct']:.1f}%")


# ============================================================================
# –ü–Ü–î–°–£–ú–û–ö
# ============================================================================

print("\n" + "=" * 70)
print("–ü–Ü–î–°–£–ú–û–ö: STRING PROCESSING –î–õ–Ø DS/DE")
print("=" * 70)

summary = """
‚úÖ REGEX PATTERNS:
   - Data extraction
   - Validation
   - Cleaning

‚úÖ DATA NORMALIZATION:
   - Company names
   - Phone numbers
   - Addresses

‚úÖ TEXT PREPROCESSING:
   - Tokenization
   - Cleaning
   - NLP –≥–æ—Ç–æ–≤–Ω—ñ—Å—Ç—å

‚úÖ VALIDATION:
   - Email, IP, dates
   - Credit cards (Luhn)
   - Custom validators

‚úÖ STRING SIMILARITY:
   - Levenshtein distance
   - Fuzzy matching
   - Deduplication

‚úÖ UNICODE:
   - Internationalization
   - Accent removal
   - Normalization

‚úÖ FORMATTING:
   - Tables
   - Reports
   - Templates

‚úÖ PRACTICAL PIPELINES:
   - End-to-end cleaning
   - Multi-step processing
   - Production-ready

üéØ –ó–ê–°–¢–û–°–£–í–ê–ù–ù–Ø –í DS/DE:
   - Data cleaning pipelines
   - Entity resolution
   - Log parsing
   - Text preprocessing
   - Data validation
   - Report generation
"""

print(summary)

print("\n‚ú® String processing - –æ—Å–Ω–æ–≤–∞ data engineering! ‚ú®\n")
